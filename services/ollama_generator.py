# services/ollama_generator.py - G√©n√©rateur de contenu avec Ollama
import requests
import json
import re
from typing import Optional, Tuple, List
from datetime import datetime

# Import conditionnel des mod√®les
try:
    from models import ContentGenerationResult, GenerationRequest, ContentTone, ImageGenerationResult
except ImportError:
    # Mod√®les de secours si models.py a des probl√®mes
    class ContentGenerationResult:
        def __init__(self, success, description=None, hashtags=None, error_message=None):
            self.success = success
            self.description = description
            self.hashtags = hashtags
            self.error_message = error_message
        
        @classmethod
        def success_result(cls, description, hashtags):
            return cls(True, description, hashtags)
        
        @classmethod
        def error_result(cls, error_message):
            return cls(False, error_message=error_message)
    
    class ImageGenerationResult:
        def __init__(self, success, image_path=None, error_message=None):
            self.success = success
            self.image_path = image_path
            self.error_message = error_message
        
        @classmethod
        def success_result(cls, image_path, prompt_used):
            return cls(True, image_path)
        
        @classmethod
        def error_result(cls, error_message, prompt_used=None):
            return cls(False, error_message=error_message)


class OllamaContentGenerator:
    """G√©n√©rateur de contenu utilisant Ollama avec Mistral en local"""
    
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "mistral:latest"):
        """
        Initialise le g√©n√©rateur avec Ollama
        
        Args:
            base_url: URL de base d'Ollama (par d√©faut localhost:11434)
            model: Nom du mod√®le √† utiliser (ex: mistral:latest, llama2, codellama)
        """
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.api_url = f"{self.base_url}/api/generate"
        
        # V√©rifier que Ollama est accessible
        try:
            self._test_connection()
            print(f"‚úÖ Connexion Ollama r√©ussie - Mod√®le: {self.model}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Attention: Ollama non accessible - {e}")
    
    def _test_connection(self):
        """Teste la connexion √† Ollama"""
        test_url = f"{self.base_url}/api/tags"
        response = requests.get(test_url, timeout=5)
        response.raise_for_status()
        
        # V√©rifier que le mod√®le existe
        models = response.json().get('models', [])
        model_names = [model['name'] for model in models]
        
        if self.model not in model_names:
            print(f"‚ö†Ô∏è  Mod√®le {self.model} non trouv√©. Mod√®les disponibles: {model_names}")
            if model_names:
                self.model = model_names[0]
                print(f"üîÑ Utilisation du mod√®le: {self.model}")
    
    def generate_description_and_hashtags(self, topic: str, tone: str = "engageant", 
                                        additional_context: str = None) -> ContentGenerationResult:
        """
        G√©n√®re une description et des hashtags pour un sujet donn√©
        
        Args:
            topic: Sujet du post
            tone: Ton du contenu
            additional_context: Contexte suppl√©mentaire
        
        Returns:
            ContentGenerationResult avec description et hashtags
        """
        try:
            prompt = self._build_generation_prompt(topic, tone, additional_context)
            
            print(f"‚úçÔ∏è  G√©n√©ration de contenu avec Ollama/Mistral pour: {topic}")
            print(f"   üé≠ Ton: {tone}")
            
            # Pr√©parer la requ√™te pour Ollama
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,  # R√©ponse compl√®te d'un coup
                "options": {
                    "temperature": 0.7,
                    "max_tokens": 500,
                    "top_p": 0.9
                }
            }
            
            # Appel √† Ollama
            response = requests.post(
                self.api_url, 
                json=payload, 
                timeout=30,
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code != 200:
                return ContentGenerationResult.error_result(f"Erreur Ollama: {response.status_code}")
            
            result = response.json()
            content = result.get('response', '')
            
            if not content:
                return ContentGenerationResult.error_result("R√©ponse vide d'Ollama")
            
            # Parser le contenu g√©n√©r√©
            description, hashtags = self._parse_generated_content(content)
            
            if description and hashtags:
                print(f"‚úÖ Contenu g√©n√©r√© avec succ√®s par Ollama")
                return ContentGenerationResult.success_result(description, hashtags)
            else:
                return ContentGenerationResult.error_result("Impossible de parser le contenu g√©n√©r√©")
                
        except requests.RequestException as e:
            error_msg = f"Erreur de connexion Ollama: {str(e)}"
            print(f"‚ùå {error_msg}")
            return ContentGenerationResult.error_result(error_msg)
            
        except Exception as e:
            error_msg = f"Erreur lors de la g√©n√©ration de contenu: {str(e)}"
            print(f"‚ùå {error_msg}")
            return ContentGenerationResult.error_result(error_msg)
    
    def _build_generation_prompt(self, topic: str, tone: str, additional_context: str = None) -> str:
        """Construit le prompt optimis√© pour Mistral"""
        
        # Prompt optimis√© pour Mistral en fran√ßais
        base_prompt = f"""Tu es un expert en marketing Instagram fran√ßais. Cr√©e une publication Instagram engageante.

SUJET: {topic}
TON: {tone}

INSTRUCTIONS:
- √âcris une description Instagram accrocheuse en fran√ßais ({tone})
- Ajoute 10-15 hashtags pertinents m√©langant fran√ßais et anglais
- Inclus un appel √† l'action subtil
- Utilise des √©mojis appropri√©s
- Maximum 150 mots pour la description

{f"CONTEXTE SUPPL√âMENTAIRE: {additional_context}" if additional_context else ""}

INSTRUCTIONS DE TON:
"""

        # Instructions sp√©cifiques selon le ton
        tone_instructions = {
            "engageant": "Sois interactif, utilise des √©mojis, pose des questions, cr√©e de l'interaction",
            "professionnel": "Reste formel mais accessible, mets l'accent sur la valeur et l'expertise",
            "d√©contract√©": "Sois naturel et familier, utilise un langage simple et accessible",
            "inspirant": "Motive et inspire, utilise des messages positifs et encourageants",
            "humoristique": "Ajoute une touche d'humour subtil et appropri√©",
            "√©ducatif": "Partage des informations utiles et des conseils pratiques"
        }
        
        if tone.lower() in tone_instructions:
            base_prompt += tone_instructions[tone.lower()]
        
        # Format de sortie strict
        base_prompt += """

FORMAT DE R√âPONSE OBLIGATOIRE (respecte exactement ce format):
DESCRIPTION: [ta description ici avec √©mojis]
HASHTAGS: [tes hashtags s√©par√©s par des espaces, commen√ßant par #]

EXEMPLE:
DESCRIPTION: D√©couvrez cette recette incroyable qui va r√©volutionner vos petits-d√©jeuners ! ü•û‚ú® Qui d'autre adore commencer la journ√©e avec de d√©licieuses cr√©ations ? Partagez vos photos en commentaires ! üëá
HASHTAGS: #cuisine #recette #petitdejeuner #healthy #foodie #instafood #cooking #breakfast #homemade #delicious #food #yummy #nutrition #lifestyle #france

Maintenant, g√©n√®re le contenu:"""
        
        return base_prompt
    
    def _parse_generated_content(self, content: str) -> Tuple[str, str]:
        """Parse le contenu g√©n√©r√© pour extraire description et hashtags"""
        try:
            lines = content.strip().split('\n')
            description = ""
            hashtags = ""
            
            for line in lines:
                line = line.strip()
                if line.startswith("DESCRIPTION:"):
                    description = line.replace("DESCRIPTION:", "").strip()
                elif line.startswith("HASHTAGS:"):
                    hashtags = line.replace("HASHTAGS:", "").strip()
                # Variations possibles
                elif line.startswith("Description:"):
                    description = line.replace("Description:", "").strip()
                elif line.startswith("Hashtags:"):
                    hashtags = line.replace("Hashtags:", "").strip()
            
            # Si pas trouv√© avec le format, essayer de parser diff√©remment
            if not description or not hashtags:
                # Chercher des patterns alternatifs
                text_parts = content.split('#')
                if len(text_parts) >= 2:
                    description = text_parts[0].strip()
                    hashtags = '#' + ' #'.join([part.split()[0] for part in text_parts[1:] if part.strip()])
            
            # Nettoyer et valider
            description = self._clean_description(description)
            hashtags = self._clean_hashtags(hashtags)
            
            return description, hashtags
            
        except Exception as e:
            print(f"‚ùå Erreur lors du parsing: {e}")
            print(f"Contenu brut: {content}")
            return "", ""
    
    def _clean_description(self, description: str) -> str:
        """Nettoie et valide la description"""
        if not description:
            return ""
        
        # Supprimer les guillemets et nettoyer
        description = description.strip('"\'').strip()
        
        # Limiter la longueur pour Instagram
        if len(description) > 2200:
            description = description[:2197] + "..."
        
        return description
    
    def _clean_hashtags(self, hashtags: str) -> str:
        """Nettoie et valide les hashtags"""
        if not hashtags:
            return ""
        
        # Extraire tous les hashtags
        hashtag_list = re.findall(r'#\w+', hashtags)
        
        # Supprimer les doublons en pr√©servant l'ordre
        seen = set()
        unique_hashtags = []
        for hashtag in hashtag_list:
            if hashtag.lower() not in seen:
                seen.add(hashtag.lower())
                unique_hashtags.append(hashtag)
        
        # Limiter √† 30 hashtags max
        unique_hashtags = unique_hashtags[:30]
        
        return " ".join(unique_hashtags)
    
    def generate_multiple_versions(self, topic: str, tone: str, count: int = 3) -> List[ContentGenerationResult]:
        """G√©n√®re plusieurs versions du contenu pour le m√™me sujet"""
        results = []
        
        for i in range(count):
            print(f"‚úçÔ∏è  G√©n√©ration version {i+1}/{count} avec Ollama")
            
            # Varier le prompt pour chaque version
            context_variations = [
                "Cr√©e une version unique et originale",
                "Propose une approche diff√©rente et cr√©ative", 
                "Offre une perspective nouvelle et engageante"
            ]
            
            additional_context = context_variations[i % len(context_variations)]
            result = self.generate_description_and_hashtags(topic, tone, additional_context)
            results.append(result)
        
        return results
    
    def get_available_models(self) -> List[str]:
        """Retourne la liste des mod√®les disponibles dans Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                return [model['name'] for model in data.get('models', [])]
            return []
        except:
            return []
    
    def switch_model(self, model_name: str) -> bool:
        """Change le mod√®le utilis√©"""
        available_models = self.get_available_models()
        if model_name in available_models:
            self.model = model_name
            print(f"üîÑ Mod√®le chang√© pour: {model_name}")
            return True
        else:
            print(f"‚ùå Mod√®le {model_name} non disponible. Mod√®les disponibles: {available_models}")
            return False


class HybridAIGenerator:
    """G√©n√©rateur hybride utilisant Ollama pour le texte et DALL-E pour les images"""
    
    def __init__(self, openai_api_key: str = None, ollama_url: str = "http://localhost:11434", 
                 ollama_model: str = "mistral:latest"):
        """
        Initialise le g√©n√©rateur hybride
        
        Args:
            openai_api_key: Cl√© OpenAI pour DALL-E (optionnel)
            ollama_url: URL d'Ollama pour le contenu texte
            ollama_model: Mod√®le Ollama √† utiliser
        """
        # Service de contenu avec Ollama
        self.content_generator = OllamaContentGenerator(ollama_url, ollama_model)
        
        # Service d'images avec OpenAI (si disponible)
        self.image_generator = None
        if openai_api_key:
            try:
                from services.ai_generator import AIImageGenerator
                self.image_generator = AIImageGenerator(openai_api_key)
                print("‚úÖ Service d'images OpenAI/DALL-E activ√©")
            except ImportError:
                print("‚ö†Ô∏è  Service d'images OpenAI non disponible")
        else:
            print("‚ÑπÔ∏è  Pas de cl√© OpenAI - Images non g√©n√©r√©es automatiquement")
    
    def generate_content(self, topic: str, tone: str = "engageant", 
                        additional_context: str = None) -> ContentGenerationResult:
        """G√©n√®re du contenu avec Ollama"""
        return self.content_generator.generate_description_and_hashtags(topic, tone, additional_context)
    
    def generate_description_and_hashtags(self, topic: str, tone: str = "engageant", 
                                        additional_context: str = None) -> ContentGenerationResult:
        """Alias pour la compatibilit√©"""
        return self.generate_content(topic, tone, additional_context)
    
    def generate_image(self, prompt: str, size: str = "1024x1024"):
        """G√©n√®re une image avec DALL-E si disponible"""
        if self.image_generator:
            return self.image_generator.generate_image(prompt, size)
        else:
            print("‚ö†Ô∏è  G√©n√©ration d'images non disponible (pas de cl√© OpenAI)")
            return ImageGenerationResult.error_result("Service d'images non disponible")
    
    def get_status(self) -> dict:
        """Retourne le statut des services"""
        return {
            'content_generator': self.content_generator is not None,
            'image_generator': self.image_generator is not None,
            'ollama_model': self.content_generator.model if self.content_generator else None,
            'available_models': self.content_generator.get_available_models() if self.content_generator else []
        }


class LocalImageGenerator:
    """
    G√©n√©rateur d'images locales - Pour une future int√©gration avec Stable Diffusion
    ou d'autres solutions locales
    """
    
    def __init__(self):
        print("‚ÑπÔ∏è  G√©n√©rateur d'images local - En d√©veloppement")
        print("üí° Pour les images, vous pouvez :")
        print("   - Utiliser DALL-E avec une cl√© OpenAI")
        print("   - Int√©grer Stable Diffusion localement")
        print("   - Uploader des images manuellement")
    
    def generate_placeholder(self, prompt: str, size: str = "1024x1024"):
        """G√©n√®re une image placeholder"""
        # Pour l'instant, retourne None
        # Dans le futur, pourrait int√©grer Stable Diffusion ou autre
        return None